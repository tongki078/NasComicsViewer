from flask import Flask, jsonify, send_from_directory, request, send_file, render_template_string, Response
import os
import urllib.parse
import unicodedata
import logging
import yaml
import time
import zipfile
import io
import sys
import sqlite3
import json
import functools
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# [ë¡œê·¸ ì„¤ì •]
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s [%(name)s] %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger("NasServer")

app = Flask(__name__)

# --- ì„¤ì • ---
BASE_PATH = "/volume2/video/GDS3/GDRIVE/READING/ë§Œí™”"

# [ì¤‘ìš” ë³€ê²½] DB íŒŒì¼ ê²½ë¡œë¥¼ ìƒëŒ€ê²½ë¡œì—ì„œ ì ˆëŒ€ê²½ë¡œë¡œ ë³€ê²½
# ìš©ëŸ‰ì´ ì•„ì£¼ ë§ì€(7.5TB ì—¬ìœ ) /volume2/video íŒŒí‹°ì…˜ì— ëª…ì‹œì ìœ¼ë¡œ ì €ì¥
METADATA_DB_PATH = '/volume2/video/NasComicsViewer_metadata_cache.db'

MAX_WORKERS = 16

# SQLite ì„ì‹œ í´ë”ë¥¼ ê³µê°„ì´ ì¶©ë¶„í•œ ê³³ìœ¼ë¡œ ê°•ì œ ì§€ì • (ë””ìŠ¤í¬ í’€ ì—ëŸ¬ ë°©ì§€)
os.environ["SQLITE_TMPDIR"] = BASE_PATH

# 3ë‹¨ê³„ êµ¬ì¡°ë¥¼ ê°€ì§„ ì¹´í…Œê³ ë¦¬ í´ë” ì´ë¦„ (ì†Œë¬¸ìë¡œ ë¹„êµ)
THREE_LEVEL_STRUCTURE_FOLDERS = ["ì™„ê²°a", "ì™„ê²°b", "ì™„ê²°", "ì‘ê°€", "ë²ˆì—­", "ì—°ì¬"]

# ì—…ë°ì´íŠ¸ ìƒíƒœë¥¼ ì €ì¥í•  ì „ì—­ ë”•ì…”ë„ˆë¦¬
update_status = {
    'is_running': False,
    'total': 0,
    'processed': 0,
    'success': 0,
    'error': 0,
    'current_item': '',
    'logs': [],
    'path': ''
}
status_lock = threading.Lock()

# --- HTML í…œí”Œë¦¿ (ê°œì„ ëœ UI) ---
ADMIN_TEMPLATE = """
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ë©”íƒ€ë°ì´í„° ê´€ë¦¬ì</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif; padding: 20px; background-color: #f8f9fa; color: #333; }
        .container { max-width: 800px; margin: 0 auto; }
        .card { background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin-bottom: 20px; }
        h1 { margin-top: 0; margin-bottom: 20px; border-bottom: 2px solid #eee; padding-bottom: 10px; font-size: 1.5em; }

        .form-group { display: flex; gap: 10px; }
        input[type="text"] { flex: 1; padding: 12px; border: 1px solid #ddd; border-radius: 6px; font-size: 16px; }
        button { padding: 12px 24px; background-color: #007bff; color: white; border: none; border-radius: 6px; cursor: pointer; font-size: 16px; font-weight: bold; transition: background 0.2s; }
        button:hover { background-color: #0056b3; }
        button:disabled { background-color: #6c757d; cursor: not-allowed; }

        .progress-container { margin-top: 20px; display: none; }
        .progress-bar-bg { width: 100%; background-color: #e9ecef; border-radius: 5px; overflow: hidden; height: 20px; margin-bottom: 10px; }
        .progress-bar { height: 100%; background-color: #28a745; width: 0%; transition: width 0.3s ease; }
        .stats { display: flex; justify-content: space-between; font-size: 0.9em; color: #666; margin-bottom: 10px; }

        .log-container { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 5px; height: 300px; overflow-y: auto; font-family: monospace; font-size: 0.85em; display: none; margin-top: 15px; }
        .log-item { margin-bottom: 4px; }
        .log-success { color: #4CAF50; }
        .log-warning { color: #FFC107; }
        .log-error { color: #F44336; }
        .log-info { color: #2196F3; }
    </style>
</head>
<body>
    <div class="container">
        <div class="card">
            <h1>ğŸ› ï¸ ë©”íƒ€ë°ì´í„° ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸</h1>
            <p style="margin-bottom: 20px; color: #666;">
                ì—…ë°ì´íŠ¸í•  í´ë”ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”. (ì˜ˆ: <code>ì™„ê²°</code>, <code>ì‘ê°€/ã„±</code>)<br>
                ë¹ˆ ì¹¸ìœ¼ë¡œ ë‘ë©´ ìµœìƒìœ„ í´ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
            </p>
            <div class="form-group">
                <input type="text" id="pathInput" placeholder="í´ë” ê²½ë¡œ ì…ë ¥...">
                <button id="startBtn" onclick="startUpdate()">ì—…ë°ì´íŠ¸ ì‹œì‘</button>
            </div>

            <div id="progressSection" class="progress-container">
                <h3 id="statusText" style="margin-top: 0; margin-bottom: 15px; font-size: 1.1em;">ì¤€ë¹„ ì¤‘...</h3>
                <div class="progress-bar-bg">
                    <div id="progressBar" class="progress-bar"></div>
                </div>
                <div class="stats">
                    <span id="progressCount">0 / 0</span>
                    <span id="percentText">0%</span>
                </div>
                <div class="stats">
                    <span style="color: #28a745;">ì„±ê³µ: <span id="successCount">0</span></span>
                    <span style="color: #dc3545;">ì‹¤íŒ¨: <span id="errorCount">0</span></span>
                </div>
            </div>

            <div id="logSection" class="log-container"></div>
        </div>
    </div>

    <script>
        let eventSource = null;

        function startUpdate() {
            const path = document.getElementById('pathInput').value;
            const btn = document.getElementById('startBtn');
            const progressSection = document.getElementById('progressSection');
            const logSection = document.getElementById('logSection');

            btn.disabled = true;
            progressSection.style.display = 'block';
            logSection.style.display = 'block';
            logSection.innerHTML = '';

            // SSE ì—°ê²° ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­)
            if (eventSource) {
                eventSource.close();
            }

            eventSource = new EventSource('/do_update_metadata?path=' + encodeURIComponent(path));

            eventSource.onmessage = function(event) {
                const data = JSON.parse(event.data);

                if (data.status === 'error') {
                    addLog(data.message, 'log-error');
                    finishUpdate();
                    return;
                }

                if (data.status === 'init') {
                    document.getElementById('statusText').innerText = `ìŠ¤ìº” ì¤€ë¹„ ì¤‘... (ì´ ${data.total}ê°œ í•­ëª© ì°¾ìŒ)`;
                    updateBars(0, data.total, 0, 0);
                    addLog(`ğŸ“¥ [INIT] Found ${data.total} items to update in '${path}'`, 'log-info');
                }
                else if (data.status === 'progress') {
                    document.getElementById('statusText').innerText = `ì²˜ë¦¬ ì¤‘: ${data.current_item}`;
                    updateBars(data.processed, data.total, data.success, data.error);

                    if (data.log) {
                        let logClass = 'log-item';
                        if (data.log.includes('âœ…')) logClass = 'log-success';
                        else if (data.log.includes('âš ï¸')) logClass = 'log-warning';
                        else if (data.log.includes('âŒ')) logClass = 'log-error';
                        addLog(data.log, logClass);
                    }
                }
                else if (data.status === 'done') {
                    document.getElementById('statusText').innerText = 'âœ¨ ì—…ë°ì´íŠ¸ ì™„ë£Œ!';
                    updateBars(data.total, data.total, data.success, data.error);
                    addLog(`âœ¨ [DONE] Finished. Updated ${data.success} items.`, 'log-info');
                    finishUpdate();
                }
            };

            eventSource.onerror = function(event) {
                console.error("EventSource failed:", event);
                addLog("âŒ ì„œë²„ì™€ì˜ ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.", "log-error");
                finishUpdate();
            };
        }

        function updateBars(processed, total, success, error) {
            const percent = total === 0 ? 0 : Math.round((processed / total) * 100);
            document.getElementById('progressBar').style.width = percent + '%';
            document.getElementById('progressCount').innerText = `${processed} / ${total}`;
            document.getElementById('percentText').innerText = `${percent}%`;
            document.getElementById('successCount').innerText = success;
            document.getElementById('errorCount').innerText = error;
        }

        function addLog(message, className) {
            const logSection = document.getElementById('logSection');
            const div = document.createElement('div');
            div.className = className;
            div.innerText = message;
            logSection.appendChild(div);
            logSection.scrollTop = logSection.scrollHeight;
        }

        function finishUpdate() {
            document.getElementById('startBtn').disabled = false;
            if (eventSource) {
                eventSource.close();
                eventSource = null;
            }
        }
    </script>
</body>
</html>
"""

# --- ìœ í‹¸ë¦¬í‹° ---
def time_it(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        logger.info(f"TIMING: {func.__name__} took {end_time - start_time:.4f} seconds to execute.")
        return result
    return wrapper

def get_db_connection():
    """DB ì—°ê²° ê³µí†µ í•¨ìˆ˜. ì„ì‹œ í´ë” ë° ì„±ëŠ¥ ì˜µì…˜ ì ìš©"""
    conn = sqlite3.connect(METADATA_DB_PATH, timeout=60)
    c = conn.cursor()
    # ê³µê°„ ë¶€ì¡± ì—ëŸ¬ í•´ê²°ì„ ìœ„í•œ ì„ì‹œ ì €ì¥ì†Œ ì§€ì • (ìµœì‹  SQLiteëŠ” pragma temp_store_directory ì§€ì› ì¤‘ë‹¨ë  ìˆ˜ ìˆìœ¼ë‚˜ í™˜ê²½ë³€ìˆ˜ì™€ ë³‘í–‰)
    try:
        c.execute(f"PRAGMA temp_store_directory = '{BASE_PATH}';")
    except Exception:
        pass

    c.execute('PRAGMA journal_mode=WAL;')
    c.execute('PRAGMA synchronous=NORMAL;')
    c.execute('PRAGMA wal_autocheckpoint=1000;')
    return conn

# --- DB ì„¤ì • ---
def init_db():
    try:
        logger.info(f"ğŸ”§ Using Database at: {METADATA_DB_PATH}")
        with get_db_connection() as conn:
            c = conn.cursor()

            c.execute('''
                CREATE TABLE IF NOT EXISTS metadata_cache (
                    path_hash TEXT PRIMARY KEY,
                    mtime REAL NOT NULL,
                    metadata_json TEXT NOT NULL,
                    cached_at REAL NOT NULL
                )
            ''')
            c.execute('''
                CREATE TABLE IF NOT EXISTS directory_cache (
                    path_hash TEXT PRIMARY KEY,
                    entries_json TEXT NOT NULL,
                    cached_at REAL NOT NULL
                )
            ''')
            conn.commit()

            c.execute("SELECT COUNT(*) FROM directory_cache")
            dir_count = c.fetchone()[0]
            c.execute("SELECT COUNT(*) FROM metadata_cache")
            meta_count = c.fetchone()[0]
            logger.info(f"ğŸ’¾ [DB_INIT] Directory Cache: {dir_count} entries, Metadata Cache: {meta_count} entries")

    except Exception as e:
        logger.error(f"âŒ [DB_INIT] Failed to initialize DB: {e}")

def get_cached_metadata(path, conn):
    path_hash = str(hash(path))
    try:
        mtime = os.path.getmtime(path)
        c = conn.cursor()
        c.execute("SELECT mtime, metadata_json FROM metadata_cache WHERE path_hash = ?", (path_hash,))
        row = c.fetchone()
        if row and row[0] == mtime:
            return json.loads(row[1])
    except Exception:
        pass
    return None

def set_cached_metadata(path, metadata, conn):
    path_hash = str(hash(path))
    retries = 3
    for attempt in range(retries):
        try:
            mtime = os.path.getmtime(path)
            metadata_json = json.dumps(metadata)
            conn.execute("INSERT OR REPLACE INTO metadata_cache (path_hash, mtime, metadata_json, cached_at) VALUES (?, ?, ?, ?)",
                          (path_hash, mtime, metadata_json, time.time()))
            return  # ì„±ê³µí•˜ë©´ ì¢…ë£Œ
        except sqlite3.OperationalError as e:
            if 'locked' in str(e).lower() and attempt < retries - 1:
                time.sleep(0.5) # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
            else:
                logger.error(f"Cache write failed for {path} after {attempt+1} attempts: {e}")
                break
        except Exception as e:
            logger.error(f"Cache write failed for {path}: {e}")
            break

def get_cached_directory_entries(path, conn):
    path_hash = str(hash(path))
    try:
        c = conn.cursor()
        c.execute("SELECT entries_json FROM directory_cache WHERE path_hash = ?", (path_hash,))
        row = c.fetchone()
        if row:
            return json.loads(row[0])
    except Exception:
        pass
    return None

def set_cached_directory_entries(path, entries, conn):
    path_hash = str(hash(path))
    retries = 3
    for attempt in range(retries):
        try:
            entries_json = json.dumps(entries)
            conn.execute("INSERT OR REPLACE INTO directory_cache (path_hash, entries_json, cached_at) VALUES (?, ?, ?)",
                          (path_hash, entries_json, time.time()))
            return
        except sqlite3.OperationalError as e:
            if 'locked' in str(e).lower() and attempt < retries - 1:
                time.sleep(0.5)
            else:
                logger.error(f"Directory cache write failed for {path}: {e}")
                break
        except Exception as e:
            logger.error(f"Directory cache write failed for {path}: {e}")
            break

# --- íŒŒì¼ ì‹œìŠ¤í…œ ë° ê²½ë¡œ ì²˜ë¦¬ ---
def normalize_nfc(s):
    if not isinstance(s, str): return s
    return unicodedata.normalize('NFC', s)

def find_actual_name_in_dir(parent, target_name):
    target_norm = normalize_nfc(target_name).lower()
    try:
        with os.scandir(parent) as it:
            for entry in it:
                if normalize_nfc(entry.name).lower() == target_norm:
                    return entry.name
    except Exception:
        pass
    return None

def get_robust_root():
    return os.path.normpath(os.path.abspath(BASE_PATH))

def resolve_actual_path(rel_path):
    root = get_robust_root()
    if not rel_path or rel_path.strip() in [".", "", "/"]:
        return root
    curr = root
    parts = urllib.parse.unquote(rel_path).replace('\\', '/').strip('/').split('/')
    for part in parts:
        if not part: continue
        actual = find_actual_name_in_dir(curr, part)
        curr = os.path.join(curr, actual) if actual else os.path.join(curr, part)
    return os.path.normpath(os.path.abspath(curr))

def is_image_file(name):
    return name and name.lower().endswith(('.jpg', '.jpeg', '.png', '.webp', '.gif'))

def clean_name(name):
    return os.path.splitext(name)[0].strip() if name else ""

# --- ë©”íƒ€ë°ì´í„° ê´€ë ¨ ë¡œì§ ---

def find_first_valid_thumb(abs_path, rel_path, files):
    first_image = None
    first_zip = None
    for f in files:
        lowered_f = f.lower()
        if is_image_file(f):
            if any(keyword in lowered_f for keyword in ["poster", "cover", "folder", "thumb"]):
                return os.path.join(rel_path, f).replace('\\', '/')
            if not first_image:
                first_image = f
        elif not first_zip and lowered_f.endswith(('.zip', '.cbz')):
            first_zip = f
    if first_image:
        return os.path.join(rel_path, first_image).replace('\\', '/')
    if first_zip:
        return "zip_thumb://" + os.path.join(rel_path, first_zip).replace('\\', '/')
    return None

def get_metadata_internal(abs_path, rel_path, conn):
    cached = get_cached_metadata(abs_path, conn)
    if cached:
        return cached

    base_name = os.path.basename(abs_path.rstrip('/\\'))
    clean_title = clean_name(normalize_nfc(base_name))
    meta = {"title": clean_title, "poster_url": None}

    if os.path.isfile(abs_path):
        if abs_path.lower().endswith(('.zip', '.cbz')):
            meta['poster_url'] = "zip_thumb://" + rel_path
        elif is_image_file(abs_path):
            meta['poster_url'] = rel_path

    set_cached_metadata(abs_path, meta, conn)
    return meta

def force_update_metadata_task(task_path, is_dir, root_path, db_path):
    conn = None
    log_msg = ""
    result = {"success": False, "title": "", "source": "NONE", "poster": None, "log": ""}

    try:
        conn = get_db_connection()

        rel_path = os.path.relpath(task_path, root_path).replace(os.sep, '/')
        base_name = os.path.basename(task_path.rstrip('/\\'))
        clean_title = clean_name(normalize_nfc(base_name))
        result["title"] = clean_title

        # [ì´ì–´í•˜ê¸° ë¡œì§] ê¸°ì¡´ì— í¬ìŠ¤í„° ì •ë³´ê°€ ì œëŒ€ë¡œ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        cached = get_cached_metadata(task_path, conn)
        if cached and cached.get('poster_url'):
            result["success"] = True
            result["source"] = "CACHE"
            result["poster"] = cached.get('poster_url')
            # ë¡œê·¸ ìƒëµ ë˜ëŠ” ê°„ë‹¨íˆ
            # result["log"] = f"â­ï¸ [SKIP] '{clean_title}' already has poster"
            return result

        meta = {"title": clean_title, "poster_url": None, "kavita_info": {}}
        source = "NONE"

        if os.path.isfile(task_path):
             source = "FILE"
             if task_path.lower().endswith(('.zip', '.cbz')):
                meta['poster_url'] = "zip_thumb://" + rel_path
             elif is_image_file(task_path):
                meta['poster_url'] = rel_path
        else:
            try:
                kavita_path = os.path.join(task_path, "kavita.yaml")
                if os.path.isfile(kavita_path):
                    with open(kavita_path, 'r', encoding='utf-8') as f:
                        kdata = yaml.safe_load(f)
                        if kdata:
                            meta['kavita_info'] = kdata
                            poster_candidates = []
                            for k in ['cover', 'poster', 'cover_image', 'coverImage']:
                                if k in kdata and kdata[k]:
                                    poster_candidates.append(kdata[k])

                            if 'search' in kdata and isinstance(kdata['search'], list) and len(kdata['search']) > 0:
                                search_item = kdata['search'][0]
                                if 'poster_url' in search_item and search_item['poster_url']:
                                    pass

                            for target in poster_candidates:
                                if os.path.exists(os.path.join(task_path, target)):
                                    meta['poster_url'] = os.path.join(rel_path, target).replace('\\', '/')
                                    source = "KAVITA_YAML"
                                    break
            except Exception as e:
                pass

            if not meta.get('poster_url'):
                try:
                    local_files = [e.name for e in os.scandir(task_path)]
                    local_files.sort(key=lambda x: normalize_nfc(x))
                    meta['poster_url'] = find_first_valid_thumb(task_path, rel_path, local_files)
                    if meta['poster_url']:
                        source = "SCAN"
                except Exception:
                    pass

        set_cached_metadata(task_path, meta, conn)
        conn.commit()

        result["success"] = True
        result["source"] = source
        result["poster"] = meta.get('poster_url')

        if meta.get('poster_url'):
             result["log"] = f"âœ… [UPDATE] '{clean_title}' updated via {source}"
        else:
             result["log"] = f"âš ï¸ [UPDATE] '{clean_title}' processed but NO POSTER found"

        return result
    except Exception as e:
        result["log"] = f"âŒ [UPDATE] Failed for {task_path}: {e}"
        return result
    finally:
        if conn: conn.close()

# --- ë¼ìš°íŠ¸: ìŠ¤ìº” ---
def process_scan_task(task_path, is_dir, root_path, db_path):
    conn = None
    meta = None
    rel_path = os.path.relpath(task_path, root_path).replace(os.sep, '/')

    try:
        conn = get_db_connection()
        meta = get_metadata_internal(task_path, rel_path, conn)
        conn.commit()
    except Exception as e:
        if conn: conn.rollback()
        logger.error(f"Error processing task {task_path}: {e}")
        # DB ì—ëŸ¬ê°€ ë°œìƒí•˜ë”ë¼ë„ ë©”íƒ€ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ë°˜í™˜í•˜ê¸° ìœ„í•œ ê¸°ë³¸ ì²˜ë¦¬
        base_name = os.path.basename(task_path.rstrip('/\\'))
        meta = {"title": clean_name(normalize_nfc(base_name)), "poster_url": None}
    finally:
        if conn: conn.close()

    return {
        'name': meta.get('title', 'Untitled'),
        'isDirectory': is_dir,
        'path': normalize_nfc(rel_path),
        'metadata': meta
    }

def scan_full_directory(abs_path, root, is_3_level_structure):
    logger.info(f"ğŸ“‚ [FS_SCAN] Scanning file system for: {abs_path}")
    all_entries = []
    scan_paths = [abs_path]
    if is_3_level_structure:
        try:
            scan_paths = [d.path for d in os.scandir(abs_path) if d.is_dir()]
        except Exception:
            pass

    tasks = []
    for current_path in scan_paths:
        try:
            with os.scandir(current_path) as it:
                for entry in it:
                    if entry.is_dir() or entry.name.lower().endswith(('.zip', '.cbz')):
                        tasks.append((entry.path, entry.is_dir()))
        except Exception:
            pass

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_map = {
            executor.submit(process_scan_task, t_path, is_dir, root, METADATA_DB_PATH):
            t_path for t_path, is_dir in tasks
        }
        for future in as_completed(future_map):
            try:
                res = future.result()
                if res: results.append(res)
            except Exception:
                pass

    paged_entry_map = {r['path']: r['name'] for r in results}
    results.sort(key=lambda r: paged_entry_map.get(r['path'], ''))

    return results

@app.route('/scan')
@time_it
def scan_comics():
    path = request.args.get('path', '')
    page = request.args.get('page', 1, type=int)
    page_size = request.args.get('page_size', 100, type=int)
    force_refresh = request.args.get('force', 'false').lower() == 'true'

    root = get_robust_root()
    abs_path = resolve_actual_path(path)
    if not os.path.isdir(abs_path):
        return jsonify({"error": "Invalid scan path"}), 404

    requested_folder_name = os.path.basename(abs_path)
    normalized_name = normalize_nfc(requested_folder_name.lower())
    is_3_level_structure = normalized_name in THREE_LEVEL_STRUCTURE_FOLDERS

    cached_entries = None
    if not force_refresh:
        try:
            with get_db_connection() as conn:
                cached_entries = get_cached_directory_entries(abs_path, conn)
        except Exception as e:
            logger.error(f"Error reading DB Cache: {e}")

    if cached_entries is not None:
        total_items = len(cached_entries)
        start_index = (page - 1) * page_size
        end_index = start_index + page_size
        paged_items = cached_entries[start_index:end_index]

        return jsonify({
            'total_items': total_items,
            'page': page,
            'page_size': page_size,
            'items': paged_items
        })

    logger.info(f"ğŸ¢ [CACHE_MISS] Scanning filesystem for '{path}'...")
    full_results = scan_full_directory(abs_path, root, is_3_level_structure)

    try:
        with get_db_connection() as conn:
            set_cached_directory_entries(abs_path, full_results, conn)
            conn.commit()
    except Exception as e:
        logger.error(f"Error writing Directory Cache for {abs_path}: {e}")

    total_items = len(full_results)
    start_index = (page - 1) * page_size
    end_index = start_index + page_size
    paged_items = full_results[start_index:end_index]

    return jsonify({
        'total_items': total_items,
        'page': page,
        'page_size': page_size,
        'items': paged_items
    })

# --- ë¼ìš°íŠ¸: ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ UI ì œê³µ ---
@app.route('/update_metadata')
def update_metadata_ui():
    # ê»ë°ê¸° HTMLë§Œ ë Œë”ë§. ë™ì‘ì€ SSE(/do_update_metadata)ë¥¼ í†µí•´ ìˆ˜í–‰ë¨
    return render_template_string(ADMIN_TEMPLATE)

# --- ë¼ìš°íŠ¸: ì‹¤ì œ ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤ (SSE) ---
@app.route('/do_update_metadata')
def do_update_metadata():
    path = request.args.get('path', '')
    root = get_robust_root()
    abs_path = resolve_actual_path(path)

    def generate():
        if not os.path.isdir(abs_path):
            yield f"data: {json.dumps({'status': 'error', 'message': f'Invalid path: {abs_path}'})}\n\n"
            return

        requested_folder_name = os.path.basename(abs_path)
        normalized_name = normalize_nfc(requested_folder_name.lower())
        is_3_level_structure = normalized_name in THREE_LEVEL_STRUCTURE_FOLDERS

        scan_paths = [abs_path]
        if is_3_level_structure:
            scan_paths = [d.path for d in os.scandir(abs_path) if d.is_dir()]

        tasks = []
        for current_path in scan_paths:
            try:
                with os.scandir(current_path) as it:
                    for entry in it:
                        if entry.is_dir() or entry.name.lower().endswith(('.zip', '.cbz')):
                            tasks.append((entry.path, entry.is_dir()))
            except Exception:
                pass

        total_tasks = len(tasks)
        yield f"data: {json.dumps({'status': 'init', 'total': total_tasks})}\n\n"

        success_count = 0
        error_count = 0
        processed_count = 0

        # ìŠ¤ë ˆë“œ í’€ í¬ê¸°ë¥¼ ì ì ˆíˆ ì¡°ì ˆ (ë„ˆë¬´ ë§ìœ¼ë©´ DB ë½ì´ ìì£¼ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_map = {
                executor.submit(force_update_metadata_task, t_path, is_dir, root, METADATA_DB_PATH):
                (t_path, is_dir) for t_path, is_dir in tasks
            }
            for future in as_completed(future_map):
                res = future.result()
                processed_count += 1

                if res.get("success"):
                    success_count += 1
                else:
                    error_count += 1

                # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì§„í–‰ ìƒíƒœì™€ ë¡œê·¸ ì „ì†¡
                payload = {
                    'status': 'progress',
                    'processed': processed_count,
                    'total': total_tasks,
                    'success': success_count,
                    'error': error_count,
                    'current_item': res.get('title', ''),
                    'log': res.get('log', '')
                }
                yield f"data: {json.dumps(payload)}\n\n"

        # ëª¨ë“  ì—…ë°ì´íŠ¸ ì™„ë£Œ í›„ directory_cache ê°±ì‹ 
        try:
            yield f"data: {json.dumps({'status': 'progress', 'processed': processed_count, 'total': total_tasks, 'success': success_count, 'error': error_count, 'current_item': 'Refreshing Directory Cache...', 'log': 'â™»ï¸ Refreshing directory cache...'})}\n\n"
            new_full_results = scan_full_directory(abs_path, root, is_3_level_structure)
            with get_db_connection() as conn:
                set_cached_directory_entries(abs_path, new_full_results, conn)
                conn.commit()
        except Exception as e:
            yield f"data: {json.dumps({'status': 'progress', 'processed': processed_count, 'total': total_tasks, 'success': success_count, 'error': error_count, 'log': f'âŒ Cache Refresh Error: {e}'})}\n\n"

        yield f"data: {json.dumps({'status': 'done', 'total': total_tasks, 'success': success_count, 'error': error_count})}\n\n"

    return Response(generate(), mimetype='text/event-stream')


# --- ê¸°íƒ€ ë¼ìš°íŠ¸ ---
@app.route('/files')
def list_files():
    p = request.args.get('path', '')
    ap = resolve_actual_path(p)
    if not os.path.exists(ap): return jsonify({"error": "Path not found", "path": ap}), 404
    if not os.path.isdir(ap): return jsonify({"error": "Not a directory", "path": ap}), 400
    items = [{'name': normalize_nfc(e.name), 'isDirectory': e.is_dir(), 'path': normalize_nfc(os.path.join(p, e.name).replace('\\', '/'))} for e in os.scandir(ap)]
    return jsonify(sorted(items, key=lambda x: x['name']))

@app.route('/download')
def download_file():
    p = request.args.get('path', '')
    if not p: return "Path missing", 400
    if p.startswith("zip_thumb://"):
        rzp = p[len("zip_thumb://"):]
        azp = resolve_actual_path(rzp)
        if os.path.isfile(azp):
            try:
                with zipfile.ZipFile(azp, 'r') as z:
                    images = sorted([n for n in z.namelist() if is_image_file(n)])
                    if images:
                        with z.open(images[0]) as f:
                            return send_file(io.BytesIO(f.read()), mimetype='image/jpeg')
            except Exception as e:
                logger.error(f"[DOWNLOAD] Zip thumb error: {azp} -> {e}")
        return "Thumbnail not found", 404
    ap = resolve_actual_path(p)
    if not os.path.isfile(ap): return "File not found", 404
    return send_from_directory(os.path.dirname(ap), os.path.basename(ap))

@app.route('/zip_entries')
def zip_entries():
    p = request.args.get('path', '')
    if not p: return "Path missing", 400
    ap = resolve_actual_path(p)
    if not os.path.isfile(ap): return jsonify({"error": "File not found"}), 404
    try:
        with zipfile.ZipFile(ap, 'r') as z:
            return jsonify(sorted([n for n in z.namelist() if is_image_file(n)]))
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/download_zip_entry')
def download_zip_entry():
    p = request.args.get('path', '')
    entry_name = request.args.get('entry', '')
    if not p or not entry_name: return "Path or entry missing", 400
    ap = resolve_actual_path(p)
    entry_name = normalize_nfc(urllib.parse.unquote(entry_name))
    if not os.path.isfile(ap): return "Zip file not found", 404
    try:
        with zipfile.ZipFile(ap, 'r') as z:
            for info in z.infolist():
                if normalize_nfc(info.filename) == entry_name:
                    with z.open(info) as f:
                        return send_file(io.BytesIO(f.read()), mimetype='application/octet-stream')
            return "Entry not found", 404
    except Exception as e:
        return f"Error reading zip: {e}", 500

@app.route('/metrics')
def metrics(): return "OK", 200

if __name__ == '__main__':
    init_db()
    logger.info("ğŸš€ ===== SERVER STARTING ===== ğŸš€")
    app.run(host='0.0.0.0', port=5555, debug=False, threaded=True, use_reloader=False)